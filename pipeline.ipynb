{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09c56223",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "808e828b",
      "metadata": {},
      "source": [
        "# Data from Kaggle https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset into data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3c0bed0",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import shutil\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"clmentbisaillon/fake-and-real-news-dataset\")\n",
        "\n",
        "# Create 'data' directory if it doesn't exist\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Move all CSV files from the dataset path to the 'data' directory\n",
        "for filename in os.listdir(path):\n",
        "    if filename.endswith('.csv'):\n",
        "        full_file_path = os.path.join(path, filename)\n",
        "        shutil.copy(full_file_path, 'data')\n",
        "\n",
        "print(\"CSV files saved to 'data/' directory.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a39a17fa",
      "metadata": {
        "id": "a39a17fa"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff477b45",
      "metadata": {
        "id": "ff477b45"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import sklearn.preprocessing._label as _l\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    precision_recall_curve,\n",
        "    average_precision_score,\n",
        ")\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.serialization import add_safe_globals\n",
        "# Own modules\n",
        "from src.data_preprocessor import TextPreprocessor, FakeNewsDataset, collate_fn\n",
        "from src.han_model import HierarchicalAttentionNetwork\n",
        "from src.model_trainer import ModelTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ddb5b25",
      "metadata": {
        "id": "1ddb5b25"
      },
      "source": [
        "## Control Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e7bfe17",
      "metadata": {
        "id": "3e7bfe17"
      },
      "outputs": [],
      "source": [
        "# CONTROL CONSTANTS\n",
        "ENABLE_TRAINING = True\n",
        "ENABLE_VALIDATION = True\n",
        "ENABLE_KFOLD_CV = True\n",
        "ENABLE_BOOTSTRAP_VALIDATION = True\n",
        "ENABLE_LEARNING_CURVE = False\n",
        "SAVE_PLOTS = True\n",
        "\n",
        "\n",
        "# CONFIGURATION CONSTANTS\n",
        "DATA_PATH = \"data/\"\n",
        "MODEL_SAVE_PATH = \"best_han_model.pth\"\n",
        "VOCAB_PATH = \"vocabulary.pkl\"\n",
        "NUM_EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 16\n",
        "TEST_SIZE = 0.2\n",
        "VALIDATION_SIZE = 0.1\n",
        "K_FOLDS = 5\n",
        "N_BOOTSTRAP = 100\n",
        "MAX_SENTENCES = 20\n",
        "MAX_WORDS_PER_SENTENCE = 50\n",
        "MAX_VOCAB_SIZE = 50000\n",
        "MIN_WORD_FREQ = 2\n",
        "PATIENCE = 3\n",
        "RANDOM_STATE = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b78d64b7",
      "metadata": {
        "id": "b78d64b7"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aecc51a3",
      "metadata": {
        "id": "aecc51a3"
      },
      "outputs": [],
      "source": [
        "# if you don't want to train the model from scratch we have run the notebook and saved it to google drive: https://drive.google.com/file/d/16HCaqszdyeXFOCiWHkKZ0ljFw0yzu4Hg/view?usp=sharing\n",
        "# so you can just use load_trained_model after you download the model and put it in this folder :)\n",
        "\n",
        "def load_trained_model(model_path=\"best_han_model.pth\", device=None):\n",
        "    \"\"\"\n",
        "    Load a trained HAN model from checkpoint.\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load checkpoint\n",
        "    add_safe_globals([_l.LabelEncoder])\n",
        "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
        "\n",
        "    # Create model with saved configuration\n",
        "    model = HierarchicalAttentionNetwork(**checkpoint[\"model_config\"])\n",
        "\n",
        "    # Load trained weights\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"Model loaded from {model_path}\")\n",
        "    print(f\"Best validation accuracy: {checkpoint.get('highest_validation_accuracy', 'no validation accuracy found')}\")\n",
        "\n",
        "    return model, checkpoint[\"model_config\"], checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5761481a",
      "metadata": {
        "id": "5761481a"
      },
      "outputs": [],
      "source": [
        "def get_model_predictions(model, data_loader, label_encoder, device=None):\n",
        "    \"\"\"\n",
        "    Get predictions for the whole dataset.\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_true_labels = []\n",
        "    all_probabilities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Getting predictions\"):\n",
        "            documents, word_lengths, sentence_lengths, labels = batch\n",
        "\n",
        "            # Move to device\n",
        "            documents = documents.to(device)\n",
        "            word_lengths = word_lengths.to(device)\n",
        "            sentence_lengths = sentence_lengths.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits, _, _ = model(documents, word_lengths, sentence_lengths)\n",
        "\n",
        "            # Get predictions and probabilities\n",
        "            probabilities = torch.softmax(logits, dim=1)\n",
        "            predictions = torch.argmax(probabilities, dim=1)\n",
        "\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_true_labels.extend(labels.numpy())\n",
        "            all_probabilities.extend(probabilities.cpu().numpy())\n",
        "\n",
        "    # Convert to label names\n",
        "    predicted_labels = label_encoder.inverse_transform(all_predictions)\n",
        "    true_labels = label_encoder.inverse_transform(all_true_labels)\n",
        "\n",
        "    return {\n",
        "        \"predictions\": predicted_labels,\n",
        "        \"true_labels\": true_labels,\n",
        "        \"probabilities\": all_probabilities,\n",
        "        \"prediction_indices\": all_predictions,\n",
        "        \"true_label_indices\": all_true_labels,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "891c8627",
      "metadata": {
        "id": "891c8627"
      },
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92686076",
      "metadata": {
        "id": "92686076"
      },
      "outputs": [],
      "source": [
        "class FakeNewsAnalyzer:\n",
        "    \"\"\"Comprehensive fake news detection analysis pipeline.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.preprocessor = None\n",
        "        self.label_encoder = None\n",
        "        self.trainset_data = None\n",
        "        self.validationset_data = None\n",
        "        self.test_data = None\n",
        "        self.model = None\n",
        "        self.training_results = None\n",
        "\n",
        "    #################\n",
        "    # Generatl utils\n",
        "    #################\n",
        "\n",
        "    def load_and_prepare_data(self):\n",
        "        print(\"Loading data\")\n",
        "        fake = pd.read_csv(os.path.join(DATA_PATH, \"Fake.csv\"))\n",
        "        true = pd.read_csv(os.path.join(DATA_PATH, \"True.csv\"))\n",
        "        fake[\"label\"] = \"fake\"\n",
        "        true[\"label\"] = \"true\"\n",
        "        data = pd.concat([fake, true], ignore_index=True)\n",
        "        trainset_data, temp_data = train_test_split(\n",
        "            data, test_size=TEST_SIZE, stratify=data[\"label\"], random_state=RANDOM_STATE\n",
        "        )\n",
        "        validationset_data, test_data = train_test_split(\n",
        "            temp_data,\n",
        "            test_size=0.5,\n",
        "            stratify=temp_data[\"label\"],\n",
        "            random_state=RANDOM_STATE,\n",
        "        )\n",
        "        self.trainset_data, self.validationset_data, self.test_data = trainset_data, validationset_data, test_data\n",
        "        if VERBOSE_OUTPUT:\n",
        "            self._print_data_info()\n",
        "        return trainset_data, validationset_data, test_data\n",
        "\n",
        "    def _print_data_info(self):\n",
        "        \"\"\"Print dataset information.\"\"\"\n",
        "        print(f\"Dataset splits:\")\n",
        "        print(f\"Train: {len(self.trainset_data)} samples\")\n",
        "        print(f\"Validation: {len(self.validationset_data)} samples\")\n",
        "        print(f\"Test: {len(self.test_data)} samples\")\n",
        "        print(\"\\nLabel distribution:\")\n",
        "        for name, data in [\n",
        "            (\"Train\", self.trainset_data),\n",
        "            (\"Validation\", self.validationset_data),\n",
        "            (\"Test\", self.test_data),\n",
        "        ]:\n",
        "            print(f\"{name} set:\")\n",
        "            print(data[\"label\"].value_counts())\n",
        "\n",
        "    def prepare_preprocessing(self):\n",
        "        print(\"Building vocabulary\")\n",
        "        self.preprocessor = TextPreprocessor(\n",
        "            maximum_vocabulary_size=MAX_VOCAB_SIZE, minimum_word_frequency=MIN_WORD_FREQ\n",
        "        )\n",
        "        self.preprocessor.build_vocabulary(self.trainset_data[\"text\"].tolist())\n",
        "        self.preprocessor.save_vocabulary(VOCAB_PATH)\n",
        "        print(\"Encoding labels\")\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.label_encoder.fit(self.trainset_data[\"label\"])\n",
        "   \n",
        "        print(f\"Vocabulary size: {self.preprocessor.vocabulary_size}\")\n",
        "        print(\n",
        "            f\"Label mapping: {dict(zip(self.label_encoder.classes_, self.label_encoder.transform(self.label_encoder.classes_)))}\"\n",
        "        )\n",
        "\n",
        "    def create_datasets_and_loaders(self, data_splits=None):\n",
        "        print(\"Creating datasets and data loaders\")\n",
        "        if data_splits is None:\n",
        "            data_splits = (self.trainset_data, self.validationset_data, self.test_data)\n",
        "        trainset_data, validationset_data, test_data = data_splits\n",
        "        # Create dataset objects\n",
        "        datasets = []\n",
        "        for data in [trainset_data, validationset_data, test_data]:\n",
        "            texts = data[\"text\"].tolist()\n",
        "            labels = self.label_encoder.transform(data[\"label\"])\n",
        "            dataset = FakeNewsDataset(\n",
        "                texts, labels, self.preprocessor, MAX_SENTENCES, MAX_WORDS_PER_SENTENCE\n",
        "            )\n",
        "            datasets.append(dataset)\n",
        "        # Create data loaders\n",
        "        trainset_loader = DataLoader(\n",
        "            datasets[0],\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_fn,\n",
        "            num_workers=2,\n",
        "        )\n",
        "        validationset_loader = DataLoader(\n",
        "            datasets[1],\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "            collate_fn=collate_fn,\n",
        "            num_workers=2,\n",
        "        )\n",
        "        test_loader = DataLoader(\n",
        "            datasets[2],\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "            collate_fn=collate_fn,\n",
        "            num_workers=2,\n",
        "        )\n",
        "        return datasets, (trainset_loader, validationset_loader, test_loader)\n",
        "\n",
        "    def trainset_model(self, trainset_loader, validationset_loader):\n",
        "        print(\"Training HAN model\")\n",
        "        trainer = ModelTrainer(\n",
        "            vocabulary_size=self.preprocessor.vocabulary_size,\n",
        "            number_of_classes=len(self.label_encoder.classes_),\n",
        "            label_encoder=self.label_encoder,\n",
        "        )\n",
        "        self.training_results = trainer.train(\n",
        "            trainset_loader=trainset_loader,\n",
        "            validationset_loader=validationset_loader,\n",
        "            number_of_epochs=NUM_EPOCHS,\n",
        "            learning_rate=LEARNING_RATE,\n",
        "            save_path=MODEL_SAVE_PATH,\n",
        "            patience=PATIENCE,\n",
        "        )\n",
        "        # Save additional metadata\n",
        "        self._save_model_metadata(trainer)\n",
        "        self.model = trainer.model\n",
        "        return trainer\n",
        "\n",
        "    def _save_model_metadata(self, trainer):\n",
        "        additional_data = {\n",
        "            \"label_encoder\": self.label_encoder,\n",
        "            \"vocabulary_size\": self.preprocessor.vocabulary_size,\n",
        "            \"number_of_classes\": len(self.label_encoder.classes_),\n",
        "            \"training_results\": self.training_results,\n",
        "        }\n",
        "        checkpoint = torch.load(MODEL_SAVE_PATH, weights_only=False)\n",
        "        checkpoint.update(additional_data)\n",
        "        torch.save(checkpoint, MODEL_SAVE_PATH)\n",
        "\n",
        "    def evaluate_model_comprehensive(self, model, validationset_loader, test_loader):\n",
        "   \n",
        "        print(\"#\" * 25)\n",
        "        print(\"Validation report\")\n",
        "        validationset_results = get_model_predictions(model, validationset_loader, self.label_encoder)\n",
        "        test_results = get_model_predictions(model, test_loader, self.label_encoder)\n",
        "        report = self._generate_performance_report(validationset_results, test_results)\n",
        "        if SAVE_PLOTS:\n",
        "            self._generate_evaluation_plots(validationset_results, test_results)\n",
        "        return report\n",
        "\n",
        "    def perform_bootstrap_validation(self, test_loader):\n",
        "        print(\"Performing bootstrap validation\")\n",
        "       \n",
        "        print(\"#\" * 25)\n",
        "        print(f\"Botstrap validation(n={N_BOOTSTRAP})\")\n",
        "\n",
        "        # Get test predictions\n",
        "        test_results = get_model_predictions(\n",
        "            self.model, test_loader, self.label_encoder\n",
        "        )\n",
        "        true_labels = np.array(test_results[\"true_labels\"])\n",
        "        predictions = np.array(test_results[\"predictions\"])\n",
        "        probabilities = np.array(test_results[\"probabilities\"])\n",
        "        bootstrap_metrics = defaultdict(list)\n",
        "        n_samples = len(true_labels)\n",
        "        for i in range(N_BOOTSTRAP):\n",
        "            if (i + 1) % 20 == 0:\n",
        "                print(f\" Bootstrap iteration {i + 1}/{N_BOOTSTRAP}\")\n",
        "            # Sample with replacement\n",
        "            bootstrap_idx = np.random.choice(n_samples, n_samples, replace=True)\n",
        "            boot_metrics = self._calculate_bootstrap_metrics(\n",
        "                true_labels[bootstrap_idx],\n",
        "                predictions[bootstrap_idx],\n",
        "                probabilities[bootstrap_idx],\n",
        "            )\n",
        "            for metric, value in boot_metrics.items():\n",
        "                bootstrap_metrics[metric].append(value)\n",
        "        bootstrap_stats = self._calculate_bootstrap_statistics(bootstrap_metrics)\n",
        "        if SAVE_PLOTS:\n",
        "            self._plot_bootstrap_distributions(bootstrap_stats)\n",
        "        return bootstrap_stats\n",
        "\n",
        "    def _calculate_bootstrap_metrics(self, true_labels, predictions, probabilities):\n",
        "        \"\"\"Calculate metrics for bootstrap sample.\"\"\"\n",
        "        probabilities = np.array(probabilities)\n",
        "        metrics = self._calculate_metrics(\n",
        "            {\"true_labels\": true_labels, \"predictions\": predictions}\n",
        "        )\n",
        "        # Add AUC for binary classification\n",
        "        if len(self.label_encoder.classes_) == 2:\n",
        "            try:\n",
        "                auc = roc_auc_score(true_labels, probabilities[:, 1])\n",
        "                metrics[\"auc\"] = auc\n",
        "            except:\n",
        "                pass  # Skip if all one class\n",
        "        return metrics\n",
        "\n",
        "    def _calculate_bootstrap_statistics(self, bootstrap_metrics):\n",
        "        \"\"\"Calculate bootstrap confidence intervals.\"\"\"\n",
        "        bootstrap_stats = {}\n",
        "        \n",
        "        print(\"\\nBOOTSTRAP CONFIDENCE INTERVALS (95%)\")\n",
        "        print(\"=\" * 40)\n",
        "        for metric, values in bootstrap_metrics.items():\n",
        "            mean_val = np.mean(values)\n",
        "            ci_lower, ci_upper = np.percentile(values, [2.5, 97.5])\n",
        "            bootstrap_stats[metric] = {\n",
        "                \"mean\": mean_val,\n",
        "                \"ci_lower\": ci_lower,\n",
        "                \"ci_upper\": ci_upper,\n",
        "                \"values\": values,\n",
        "            }\n",
        "            \n",
        "            print(\n",
        "                f\"{metric.capitalize()}: {mean_val:.4f} [{ci_lower:.4f}, {ci_upper:.4f}]\"\n",
        "            )\n",
        "        return bootstrap_stats\n",
        "\n",
        "    def perform_kfold_validation(self):\n",
        "        print(\"Initializing k-fold cross validation\")\n",
        "        \n",
        "        print(\"#\" * 25)\n",
        "        print(f\"K fold validation (k={K_FOLDS})\")\n",
        "        \n",
        "        # Combine all data for k-fold\n",
        "        full_data = pd.concat([self.trainset_data, self.validationset_data], ignore_index=True)\n",
        "        texts = full_data[\"text\"].tolist()\n",
        "        labels = self.label_encoder.transform(full_data[\"label\"])\n",
        "        full_dataset = FakeNewsDataset(\n",
        "            texts, labels, self.preprocessor, MAX_SENTENCES, MAX_WORDS_PER_SENTENCE\n",
        "        )\n",
        "        # Perform k-fold\n",
        "        kfold = StratifiedKFold(\n",
        "            n_splits=K_FOLDS, shuffle=True, random_state=RANDOM_STATE\n",
        "        )\n",
        "        fold_results = []\n",
        "        fold_metrics = defaultdict(list)\n",
        "        for fold, (trainset_idx, validationset_idx) in enumerate(kfold.split(texts, labels)):\n",
        "  \n",
        "            print(f\"\\nFold {fold + 1}/{K_FOLDS}\")\n",
        "            fold_result = self._trainset_and_evaluate_fold(\n",
        "                full_dataset, trainset_idx, validationset_idx, fold\n",
        "            )\n",
        "            fold_results.append(fold_result)\n",
        "            # Store metrics for statistics\n",
        "            for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n",
        "                fold_metrics[metric].append(fold_result[metric])\n",
        "        cv_stats = self._calculate_cv_statistics(fold_metrics)\n",
        "        if SAVE_PLOTS:\n",
        "            self._plot_kfold_results(fold_results, cv_stats)\n",
        "        return fold_results, cv_stats\n",
        "\n",
        "    def _trainset_and_evaluate_fold(self, full_dataset, trainset_idx, validationset_idx, fold):\n",
        "        \"\"\"Train and evaluate a single fold.\"\"\"\n",
        "        # Create fold datasets\n",
        "        trainset_subset = Subset(full_dataset, trainset_idx)\n",
        "        validationset_subset = Subset(full_dataset, validationset_idx)\n",
        "        # Create data loaders\n",
        "        trainset_loader = DataLoader(\n",
        "            trainset_subset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_fn,\n",
        "            num_workers=2,\n",
        "        )\n",
        "        validationset_loader = DataLoader(\n",
        "            validationset_subset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "            collate_fn=collate_fn,\n",
        "            num_workers=2,\n",
        "        )\n",
        "        # Train model\n",
        "        trainer = ModelTrainer(\n",
        "            vocabulary_size=self.preprocessor.vocabulary_size,\n",
        "            number_of_classes=len(self.label_encoder.classes_),\n",
        "            label_encoder=self.label_encoder,\n",
        "        )\n",
        "        fold_epochs = max(5, NUM_EPOCHS // 2)\n",
        "        trainer.train(\n",
        "            trainset_loader=trainset_loader,\n",
        "            validationset_loader=validationset_loader,\n",
        "            number_of_epochs=fold_epochs,\n",
        "            learning_rate=LEARNING_RATE,\n",
        "            save_path=None,\n",
        "            patience=2,\n",
        "        )\n",
        "        # Evaluate fold\n",
        "        validationset_results = get_model_predictions(\n",
        "            trainer.model, validationset_loader, self.label_encoder\n",
        "        )\n",
        "        metrics = self._calculate_metrics(validationset_results)\n",
        "        fold_result = {\"fold\": fold + 1, **metrics}\n",
        "        \n",
        "        print(\n",
        "            f\"Fold {fold + 1} Results: Acc={metrics['accuracy']:.4f}, F1={metrics['f1_score']:.4f}\"\n",
        "        )\n",
        "        return fold_result\n",
        "\n",
        "    def _calculate_cv_statistics(self, fold_metrics):\n",
        "        print(\"Computing cross-validation statistics\")\n",
        "        cv_stats = {}\n",
        "     \n",
        "        print(\"#\" * 25)\n",
        "        print(\"Cross validation\")\n",
        "     \n",
        "        for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n",
        "            values = fold_metrics[metric]\n",
        "            mean_val, std_val = np.mean(values), np.std(values)\n",
        "            cv_stats[metric] = {\n",
        "                \"mean\": mean_val,\n",
        "                \"std\": std_val,\n",
        "                \"min\": min(values),\n",
        "                \"max\": max(values),\n",
        "                \"values\": values,\n",
        "            }\n",
        "           \n",
        "            print(f\"{metric.capitalize()}: {mean_val:.4f} ± {std_val:.4f}\")\n",
        "        return cv_stats\n",
        "\n",
        "\n",
        "    ######################\n",
        "    # Evaluation pipeline\n",
        "    ######################\n",
        "    def _generate_performance_report(self, validationset_results, test_results):\n",
        "        print(\"Computing performance metrics\")\n",
        "        report = {}\n",
        "        # Basic metrics\n",
        "        for name, results in [(\"Validation\", validationset_results), (\"Test\", test_results)]:\n",
        "            metrics = self._calculate_metrics(results)\n",
        "            report[f\"{name.lower()}_metrics\"] = metrics\n",
        "        \n",
        "            print(f\"\\n{name} Set Performance:\")\n",
        "            for metric, value in metrics.items():\n",
        "                print(f\" {metric.capitalize()}: {value:.4f}\")\n",
        "        # ROC AUC for binary classification\n",
        "        if len(self.label_encoder.classes_) == 2:\n",
        "            validationset_probs = np.array(validationset_results[\"probabilities\"])\n",
        "            test_probs = np.array(test_results[\"probabilities\"])\n",
        "            try:\n",
        "                validationset_auc = roc_auc_score(validationset_results[\"true_labels\"], validationset_probs[:, 1])\n",
        "                test_auc = roc_auc_score(test_results[\"true_labels\"], test_probs[:, 1])\n",
        "            except ValueError:\n",
        "                validationset_auc = float(\"nan\")\n",
        "                test_auc = float(\"nan\")\n",
        "            report.update({\"validationset_auc\": validationset_auc, \"test_auc\": test_auc})\n",
        "         \n",
        "            print(f\"\\nROC AUC - Validation: {validationset_auc:.4f}, Test: {test_auc:.4f}\")\n",
        "        # Overfitting analysis\n",
        "        overfitting_gap = abs(\n",
        "            report[\"validation_metrics\"][\"accuracy\"]\n",
        "            - report[\"test_metrics\"][\"accuracy\"]\n",
        "        )\n",
        "        report[\"overfitting_gap\"] = overfitting_gap\n",
        "      \n",
        "        print(f\"\\nOverfitting gap: {overfitting_gap:.4f}\")\n",
        "        return report\n",
        "\n",
        "    def _calculate_metrics(self, results):\n",
        "        print(\"Computing standard classification metrics\")\n",
        "        accuracy = accuracy_score(results[\"true_labels\"], results[\"predictions\"])\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            results[\"true_labels\"], results[\"predictions\"], average=\"weighted\"\n",
        "        )\n",
        "        return {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1_score\": f1,\n",
        "        }\n",
        "\n",
        "    def _generate_evaluation_plots(self, validationset_results, test_results):\n",
        "        print(\"Generating evaluation plots\")\n",
        "        if len(self.label_encoder.classes_) == 2:\n",
        "            self._plot_roc_curves(validationset_results, test_results)\n",
        "            self._plot_precision_recall_curves(validationset_results, test_results)\n",
        "        self._plot_confusion_matrices(validationset_results, test_results)\n",
        "        if self.training_results and SAVE_PLOTS:\n",
        "            self._plot_training_history()\n",
        "\n",
        "    #############\n",
        "    # Plot utils\n",
        "    #############\n",
        "    def _plot_roc_curves(self, validationset_results, test_results):\n",
        "        \"\"\"Plot ROC curves for both validation and test sets.\"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "        for ax, results, title in [\n",
        "            (ax1, validationset_results, \"Validation\"),\n",
        "            (ax2, test_results, \"Test\"),\n",
        "        ]:\n",
        "            # 1) Convert probability lists to a NumPy array so we can slice\n",
        "            probs = np.array(results[\"probabilities\"])  # shape (N, 2)\n",
        "            scores = probs[:, 1]  # “score” for class 1\n",
        "            # 2) Convert y_true into a binary 0/1 array\n",
        "            y_true_raw = np.array(results[\"true_labels\"])\n",
        "            if y_true_raw.dtype.kind in (\"U\", \"S\", \"O\"):  # string or object dtype\n",
        "                # Assume pos_label is \"true\"\n",
        "                y_true = (y_true_raw == \"true\").astype(int)\n",
        "            else:\n",
        "                # Already numeric (0/1)\n",
        "                y_true = y_true_raw.astype(int)\n",
        "            # 3) Compute ROC curve and AUC\n",
        "            fpr, tpr, _ = roc_curve(y_true, scores)\n",
        "            auc = roc_auc_score(y_true, scores)\n",
        "            # 4) Plot\n",
        "            ax.plot(fpr, tpr, label=f\"{title} (AUC = {auc:.3f})\")\n",
        "            ax.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
        "            ax.set_xlabel(\"False Positive Rate\")\n",
        "            ax.set_ylabel(\"True Positive Rate\")\n",
        "            ax.set_title(f\"{title} ROC Curve\")\n",
        "            ax.legend()\n",
        "            ax.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"./plots/roc_curves.png\", dpi=300, bbox_inches=\"tight\")\n",
        "        plt.show()\n",
        "\n",
        "    def _plot_precision_recall_curves(self, validationset_results, test_results):\n",
        "        \"\"\"Plot Precision–Recall curves for validation and test sets.\"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "        for ax, results, title in [\n",
        "            (ax1, validationset_results, \"Validation\"),\n",
        "            (ax2, test_results, \"Test\"),\n",
        "        ]:\n",
        "            # 1) Convert probability lists to a NumPy array so we can slice\n",
        "            probs = np.array(results[\"probabilities\"])  # shape (N, 2)\n",
        "            scores = probs[:, 1]  # “score” for class-1\n",
        "            # 2) Convert y_true into a binary 0/1 array\n",
        "            y_true_raw = np.array(results[\"true_labels\"])\n",
        "            if y_true_raw.dtype.kind in (\"U\", \"S\", \"O\"):  # string or object dtype\n",
        "                # Assume positive class is \"true\"\n",
        "                y_true = (y_true_raw == \"true\").astype(int)\n",
        "            else:\n",
        "                # Already numeric (0/1)\n",
        "                y_true = y_true_raw.astype(int)\n",
        "            # 3) Compute Precision–Recall curve and Average Precision\n",
        "            precision, recall, _ = precision_recall_curve(y_true, scores)\n",
        "            ap = average_precision_score(y_true, scores)\n",
        "            # 4) Plot\n",
        "            ax.plot(recall, precision, label=f\"{title} (AP = {ap:.3f})\")\n",
        "            ax.set_xlabel(\"Recall\")\n",
        "            ax.set_ylabel(\"Precision\")\n",
        "            ax.set_title(f\"{title} Precision–Recall Curve\")\n",
        "            ax.legend()\n",
        "            ax.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"./plots/precision_recall_curves.png\", dpi=300, bbox_inches=\"tight\")\n",
        "        plt.show()\n",
        "\n",
        "    def _plot_confusion_matrices(self, validationset_results, test_results):\n",
        "        \"\"\"Plot confusion matrices for validation and test sets.\"\"\"\n",
        "        # Ensure label_encoder is set on self\n",
        "        if not hasattr(self, \"label_encoder\"):\n",
        "            raise AttributeError(\n",
        "                \"self.label_encoder must be defined before calling _plot_confusion_matrices\"\n",
        "            )\n",
        "        # Class names come from the encoder (e.g. [\"fake\", \"true\"])\n",
        "        class_names = list(self.label_encoder.classes_)\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "        for ax, results, title, cmap in [\n",
        "            (ax1, validationset_results, \"Validation\", \"Blues\"),\n",
        "            (ax2, test_results, \"Test\", \"Oranges\"),\n",
        "        ]:\n",
        "            y_true = np.array(results[\"true_labels\"])\n",
        "            y_pred = np.array(results[\"predictions\"])\n",
        "            # Decide whether labels are numeric or string:\n",
        "            if y_true.dtype.kind in (\"U\", \"S\", \"O\") or y_pred.dtype.kind in (\n",
        "                \"U\",\n",
        "                \"S\",\n",
        "                \"O\",\n",
        "            ):\n",
        "                # String case: force labels to [\"fake\",\"true\"]\n",
        "                labels_to_use = class_names\n",
        "            else:\n",
        "                # Numeric case: force labels to [0,1]\n",
        "                labels_to_use = [0, 1]\n",
        "            # Now compute a 2×2 confusion matrix with the chosen labels list\n",
        "            cm = confusion_matrix(y_true, y_pred, labels=labels_to_use)\n",
        "            sns.heatmap(\n",
        "                cm,\n",
        "                annot=True,\n",
        "                fmt=\"d\",\n",
        "                cmap=cmap,\n",
        "                xticklabels=labels_to_use,\n",
        "                yticklabels=labels_to_use,\n",
        "                ax=ax,\n",
        "            )\n",
        "            ax.set_title(f\"{title} Confusion Matrix\")\n",
        "            ax.set_xlabel(\"Predicted\")\n",
        "            ax.set_ylabel(\"Actual\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"./plots/confusion_matrices.png\", dpi=300, bbox_inches=\"tight\")\n",
        "        plt.show()\n",
        "\n",
        "    def _plot_training_history(self):\n",
        "        \"\"\"Plot training history.\"\"\"\n",
        "        if \"trainset_losses\" not in self.training_results:\n",
        "            return\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "        # Plot losses\n",
        "        ax1.plot(\n",
        "            self.training_results[\"trainset_losses\"], label=\"Training Loss\", marker=\"o\"\n",
        "        )\n",
        "        ax1.plot(\n",
        "            self.training_results[\"validationset_losses\"], label=\"Validation Loss\", marker=\"s\"\n",
        "        )\n",
        "        ax1.set_title(\"Training and Validation Loss\")\n",
        "        ax1.set_xlabel(\"Epoch\")\n",
        "        ax1.set_ylabel(\"Loss\")\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "        # Plot accuracies\n",
        "        if (\n",
        "            \"trainset_accuracies\" in self.training_results\n",
        "            and \"validationset_accuracyuracies\" in self.training_results\n",
        "        ):\n",
        "            ax2.plot(\n",
        "                self.training_results[\"trainset_accuracies\"],\n",
        "                label=\"Training Accuracy\",\n",
        "                marker=\"o\",\n",
        "            )\n",
        "            ax2.plot(\n",
        "                self.training_results[\"validationset_accuracyuracies\"],\n",
        "                label=\"Validation Accuracy\",\n",
        "                marker=\"s\",\n",
        "            )\n",
        "            ax2.set_title(\"Training and Validation Accuracy\")\n",
        "            ax2.set_xlabel(\"Epoch\")\n",
        "            ax2.set_ylabel(\"Accuracy\")\n",
        "            ax2.legend()\n",
        "            ax2.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"./plots/training_history.png\", dpi=300, bbox_inches=\"tight\")\n",
        "        plt.show()\n",
        "\n",
        "    def _plot_kfold_results(self, fold_results, cv_stats):\n",
        "        \"\"\"Plot k-fold results.\"\"\"\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        metrics = [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]\n",
        "        titles = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
        "        axes = [ax1, ax2, ax3, ax4]\n",
        "        for metric, title, ax in zip(metrics, titles, axes):\n",
        "            values = cv_stats[metric][\"values\"]\n",
        "            folds = range(1, len(values) + 1)\n",
        "            mean_val = cv_stats[metric][\"mean\"]\n",
        "            std_val = cv_stats[metric][\"std\"]\n",
        "            ax.plot(folds, values, \"bo-\", label=\"Fold Results\")\n",
        "            ax.axhline(\n",
        "                y=mean_val, color=\"r\", linestyle=\"--\", label=f\"Mean = {mean_val:.3f}\"\n",
        "            )\n",
        "            ax.fill_between(\n",
        "                folds,\n",
        "                [mean_val - std_val] * len(folds),\n",
        "                [mean_val + std_val] * len(folds),\n",
        "                alpha=0.2,\n",
        "                color=\"red\",\n",
        "                label=f\"±1 Std\",\n",
        "            )\n",
        "            ax.set_xlabel(\"Fold\")\n",
        "            ax.set_ylabel(title)\n",
        "            ax.set_title(f\"K-Fold {title}\")\n",
        "            ax.legend()\n",
        "            ax.grid(True)\n",
        "            ax.set_xticks(folds)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"./plots/kfold_results.png\", dpi=300, bbox_inches=\"tight\")\n",
        "        plt.show()\n",
        "\n",
        "    def _plot_bootstrap_distributions(self, bootstrap_stats):\n",
        "        \"\"\"Plot bootstrap distributions.\"\"\"\n",
        "        n_metrics = min(len(bootstrap_stats), 4)\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        axes = axes.flatten()\n",
        "        for i, (metric, stats) in enumerate(list(bootstrap_stats.items())[:4]):\n",
        "            values = stats[\"values\"]\n",
        "            ax = axes[i]\n",
        "            ax.hist(values, bins=30, alpha=0.7, color=\"skyblue\", edgecolor=\"black\")\n",
        "            mean_val = stats[\"mean\"]\n",
        "            ci_lower, ci_upper = stats[\"ci_lower\"], stats[\"ci_upper\"]\n",
        "            ax.axvline(\n",
        "                mean_val,\n",
        "                color=\"red\",\n",
        "                linestyle=\"--\",\n",
        "                linewidth=2,\n",
        "                label=f\"Mean = {mean_val:.3f}\",\n",
        "            )\n",
        "            ax.axvline(ci_lower, color=\"orange\", linestyle=\":\", linewidth=2)\n",
        "            ax.axvline(ci_upper, color=\"orange\", linestyle=\":\", linewidth=2)\n",
        "            ax.fill_betweenx(\n",
        "                [0, ax.get_ylim()[1]],\n",
        "                ci_lower,\n",
        "                ci_upper,\n",
        "                alpha=0.3,\n",
        "                color=\"orange\",\n",
        "                label=f\"95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]\",\n",
        "            )\n",
        "            ax.set_xlabel(metric.capitalize())\n",
        "            ax.set_ylabel(\"Frequency\")\n",
        "            ax.set_title(f\"Bootstrap Distribution: {metric.capitalize()}\")\n",
        "            ax.legend()\n",
        "            ax.grid(True, alpha=0.3)\n",
        "        # Hide unused subplots\n",
        "        for i in range(n_metrics, 4):\n",
        "            axes[i].set_visible(False)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"./plots/bootstrap_distributions.png\", dpi=300, bbox_inches=\"tight\")\n",
        "        plt.show()\n",
        "\n",
        "    ##################\n",
        "    # Actual pipeline\n",
        "    ##################\n",
        "\n",
        "    def run_complete_analysis(self):\n",
        "        print(\"Starting Fake News Detection Analysis Pipeline\")\n",
        "        validation_report = None\n",
        "        kfold_results = cv_stats = None\n",
        "        bootstrap_stats = None\n",
        "        # 1. Data preparation\n",
        "        self.load_and_prepare_data()\n",
        "        self.prepare_preprocessing()\n",
        "        # 2. Data loaders\n",
        "        datasets, (trainset_loader, validationset_loader, test_loader) = (self.create_datasets_and_loaders())\n",
        "        # 3. Model training\n",
        "        if ENABLE_TRAINING:\n",
        "            trainer = self.trainset_model(trainset_loader, validationset_loader)\n",
        "        else:\n",
        "            # Load existing model and vocabulary\n",
        "            self.model, _, checkpoint = load_trained_model(MODEL_SAVE_PATH)\n",
        "            print(\"Checkpoint keys:\", checkpoint.keys())\n",
        "            self.preprocessor = TextPreprocessor()\n",
        "            self.preprocessor.load_vocabulary(VOCAB_PATH)\n",
        "            self.label_encoder = checkpoint[\"label_encoder\"]\n",
        "        # 4. Comprehensive validation\n",
        "        if ENABLE_VALIDATION:\n",
        "            validation_report = self.evaluate_model_comprehensive(\n",
        "                self.model, validationset_loader, test_loader\n",
        "            )\n",
        "        # 5. K-fold cross validations\n",
        "        if ENABLE_KFOLD_CV:\n",
        "            kfold_results, cv_stats = self.perform_kfold_validation()\n",
        "        # 6. Bootstrap validation\n",
        "        if ENABLE_BOOTSTRAP_VALIDATION:\n",
        "            bootstrap_stats = self.perform_bootstrap_validation(test_loader)\n",
        "        return {\n",
        "            \"validation_report\": validation_report if ENABLE_VALIDATION else None,\n",
        "            \"kfold_results\": (kfold_results, cv_stats) if ENABLE_KFOLD_CV else None,\n",
        "            \"bootstrap_stats\": bootstrap_stats if ENABLE_BOOTSTRAP_VALIDATION else None,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acad0f04",
      "metadata": {
        "id": "acad0f04",
        "outputId": "ba88f7df-9bab-4aec-ea39-5def88e046bf"
      },
      "outputs": [],
      "source": [
        "# Initialize analyzer\n",
        "analyzer = FakeNewsAnalyzer()\n",
        "\n",
        "# Run complete analysis\n",
        "results = analyzer.run_complete_analysis()\n",
        "\n",
        "print(\"\\nRESULTS\\n\")\n",
        "print(results)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
